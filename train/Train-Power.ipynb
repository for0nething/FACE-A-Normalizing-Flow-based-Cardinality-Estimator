{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Change it to the project root path \"\"\"\n",
    "PROJECT_PATH = '/home/jiayi/disk/FACE-release/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_ID = 1\n",
    "dataset_name = 'power'\n",
    "ID = 2\n",
    "\n",
    "ckpts_PATH = PROJECT_PATH + 'train/models/{}/'.format(dataset_name)\n",
    "data_PATH  = PROJECT_PATH + 'data/'\n",
    "\n",
    "\"\"\" network parameters\"\"\"\n",
    "hidden_features = 108\n",
    "num_flow_steps = 6\n",
    "train_batch_size = 512\n",
    "learning_rate = 0.0005\n",
    "monitor_interval = 5000\n",
    "\n",
    "\n",
    "anneal_learning_rate = True\n",
    "base_transform_type = 'rq-coupling'\n",
    "\n",
    "dropout_probability = 0\n",
    "grad_norm_clip_value = 5.\n",
    "linear_transform_type='lu'\n",
    "\n",
    "num_bins = 8\n",
    "num_training_steps = 400000\n",
    "num_transform_blocks = 2\n",
    "seed = 1638128\n",
    "tail_bound = 3\n",
    "use_batch_norm = False\n",
    "\n",
    "val_batch_size = 262144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prefetcher as pf\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "# from tensorboardX import SummaryWriter\n",
    "from time import sleep\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from nflows import transforms\n",
    "from nflows import distributions\n",
    "from nflows import utils\n",
    "from nflows import flows\n",
    "import nflows.nn as nn_\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(GPU_ID)\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class  PowerDataset(Dataset):\n",
    "    def __init__(self, split='train', frac=None):\n",
    "        path = os.path.join(data_PATH, '{}.npy'.format(dataset_name))\n",
    "        self.data = np.load(path).astype(np.float32)\n",
    "        print('data shape:', self.data.shape)\n",
    "\n",
    "        self.n, self.dim = self.data.shape\n",
    "        if frac is not None:\n",
    "            self.n = int(frac * self.n)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = time.time()\n",
    "train_dataset = PowerDataset()\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = train_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_dataset = PowerDataset()\n",
    "val_loader = data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "train_loader = list(train_loader)\n",
    "val_loader = list(val_loader)\n",
    "TRAIN_LOADER_LEN = len(train_loader)\n",
    "\n",
    "\n",
    "features = train_dataset.dim\n",
    "\n",
    "print('Load data took [{}] s'.format(time.time() - st_time))\n",
    "print('train loader length is [{}]'.format(TRAIN_LOADER_LEN))\n",
    "print('val loader length is [{}]'.format(len(val_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp():\n",
    "    formatted_time = time.strftime('%d-%b-%y||%H:%M:%S')\n",
    "    return formatted_time\n",
    "timestamp = get_timestamp()\n",
    "print('Timestamp is ', timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_transform():\n",
    "    if linear_transform_type == 'permutation':\n",
    "        return transforms.RandomPermutation(features=features)\n",
    "    elif linear_transform_type == 'lu':\n",
    "        return transforms.CompositeTransform([\n",
    "            transforms.RandomPermutation(features=features),\n",
    "            transforms.LULinear(features, identity_init=True)\n",
    "        ])\n",
    "    elif linear_transform_type == 'svd':\n",
    "        return transforms.CompositeTransform([\n",
    "            transforms.RandomPermutation(features=features),\n",
    "            transforms.SVDLinear(features, num_householder=10, identity_init=True)\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_base_transform(i):\n",
    "    # tmp_mask = utils.create_alternating_binary_mask(features, even=(i % 2 == 0))\n",
    "    return transforms.coupling.PiecewiseRationalQuadraticCouplingTransform(\n",
    "        mask=utils.create_alternating_binary_mask(features, even=(i % 2 == 0)),\n",
    "        transform_net_create_fn=lambda in_features, out_features: nn_.nets.ResidualNet(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            hidden_features=hidden_features,\n",
    "            context_features=None,\n",
    "            num_blocks=num_transform_blocks,\n",
    "            activation=F.relu,\n",
    "            dropout_probability=dropout_probability,\n",
    "            use_batch_norm=use_batch_norm\n",
    "        ),\n",
    "        num_bins=num_bins,\n",
    "        tails='linear',\n",
    "        tail_bound=tail_bound,\n",
    "        apply_unconditional_transform=True\n",
    "    )\n",
    "\n",
    "\n",
    "# torch.masked_select()\n",
    "def create_transform():\n",
    "    transform = transforms.CompositeTransform([\n",
    "        transforms.CompositeTransform([\n",
    "            create_linear_transform(),\n",
    "            create_base_transform(i)\n",
    "        ]) for i in range(num_flow_steps)\n",
    "    ] + [\n",
    "        create_linear_transform()\n",
    "    ])\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "distribution = distributions.StandardNormal((features,))\n",
    "transform = create_transform()\n",
    "flow = flows.Flow(transform, distribution).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = utils.get_num_parameters(flow)\n",
    "print('There are {} trainable parameters in this model.'.format(n_params))\n",
    "print('Parameters total size is {} MB'.format(n_params * 4 / 1024 / 1024))\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters(), lr=learning_rate)\n",
    "if anneal_learning_rate:\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, np.ceil(num_training_steps / TRAIN_LOADER_LEN) , 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_score = -1e10\n",
    "prefetcher = pf.data_prefetcher(train_loader)\n",
    "\n",
    "num_training_steps = int(np.ceil(num_training_steps/TRAIN_LOADER_LEN) * TRAIN_LOADER_LEN)\n",
    "\n",
    "print('num training steps is ', num_training_steps)\n",
    "for step in range(num_training_steps):\n",
    "    if step % 10000 == 0:\n",
    "        print('[{}] {}/400000  {}% has finished!'.format(datetime.datetime.now(), step, 100.*step/400000))\n",
    "\n",
    "    flow.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch = prefetcher.next()\n",
    "    if batch is None:\n",
    "        prefetcher = pf.data_prefetcher(train_loader)\n",
    "        batch = prefetcher.next()\n",
    "\n",
    "    log_density = flow.log_prob(batch)\n",
    "    loss = - torch.mean(log_density)\n",
    "    loss.backward()\n",
    "    if grad_norm_clip_value is not None:\n",
    "        clip_grad_norm_(flow.parameters(), grad_norm_clip_value)\n",
    "    optimizer.step()\n",
    "\n",
    "    if (step + 1) % monitor_interval == 0:\n",
    "        flow.eval()\n",
    "        val_prefetcher = pf.data_prefetcher(val_loader)\n",
    "        with torch.no_grad():\n",
    "            running_val_log_density = 0\n",
    "            while True:\n",
    "                val_batch = val_prefetcher.next()\n",
    "                if val_batch is None:\n",
    "                    break\n",
    "\n",
    "                log_density_val = flow.log_prob(val_batch.to(device).detach())\n",
    "                mean_log_density_val = torch.mean(log_density_val).detach()\n",
    "                running_val_log_density += mean_log_density_val\n",
    "            running_val_log_density /= len(val_loader)\n",
    "            print('[{}] step now is [{:6d}] running_val_log_density is {:.4f}'.format(datetime.datetime.now(), step, running_val_log_density), end='')\n",
    "\n",
    "        if running_val_log_density > best_val_score:\n",
    "            best_val_score = running_val_log_density\n",
    "            print('  ## New best! ##')\n",
    "            path = os.path.join(ckpts_PATH,\n",
    "                                '{}-id{}-best-val.t'.format(dataset_name, ID))\n",
    "            torch.save(flow.state_dict(), path)\n",
    "        else:\n",
    "            print('')\n",
    "    \n",
    "\n",
    "    if (step + 1) % 20000 == 0 :\n",
    "        flow.eval()\n",
    "        print('[{}] save once. Step is {} best val score is {}'.format(datetime.datetime.now(), step, best_val_score))\n",
    "\n",
    "\n",
    "        path = os.path.join(ckpts_PATH,\n",
    "                            '{}-id{}-step-{}.t'.format(dataset_name, ID, step + 1))\n",
    "        torch.save(flow.state_dict(), path)\n",
    "        \n",
    "    if anneal_learning_rate and (step + 1) % TRAIN_LOADER_LEN == 0:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ACE]",
   "language": "python",
   "name": "conda-env-.conda-ACE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
